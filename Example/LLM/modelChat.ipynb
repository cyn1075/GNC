{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533bc860-6b36-4c1c-9caa-9e327f4e0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a952801-5e95-47c6-9b65-457631e11026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        input_ids = self.tokenizer.encode(text, truncation=True, max_length=self.max_length, padding='max_length')\n",
    "        return torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    return torch.stack(batch, dim=0)\n",
    "\n",
    "# 데이터 준비\n",
    "texts = [\"Hello, how are you?\", \"I'm doing great, thanks!\"]\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "dataset = MyDataset(texts, tokenizer, max_length=20)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=my_collate_fn)\n",
    "\n",
    "# 모델 정의\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# 손실 함수 및 최적화 기법 설정\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# 훈련 루프\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch.to(torch.device('cuda'))  # GPU를 사용하려면 이 부분을 적절히 수정하세요.\n",
    "        outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/3], Loss: {loss.item()}')\n",
    "\n",
    "# 테스트 예제\n",
    "test_text = \"How are you today?\"\n",
    "input_ids = tokenizer.encode(test_text, return_tensors='pt')\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=3, no_repeat_ngram_size=2, early_stopping=True)\n",
    "\n",
    "for i, sample_output in enumerate(output):\n",
    "    print(f\"Example {i+1}: {tokenizer.decode(sample_output, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be460396-0eb5-4d97-8af7-0b6391b93037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
