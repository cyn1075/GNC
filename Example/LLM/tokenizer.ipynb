{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d5af4-4eb9-4343-8098-488ac139acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# BERT 모델과 토크나이저 불러오기 (OPTION 1)\n",
    "'''\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name, output_attentions=True)\n",
    "'''\n",
    "\n",
    "# BioBERT 모델과 토크나이저 불러오기 (OPTION 2)\n",
    "model_name = 'dmis-lab/biobert-base-cased-v1.1'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name, output_attentions=True)\n",
    "\n",
    "# 입력 문장\n",
    "sentence = \"Find diseases associated with glucose\"\n",
    "\n",
    "# 토큰화\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "# 모델을 통해 출력값과 attention 가중치 계산\n",
    "outputs = model(**inputs)\n",
    "attentions = outputs.attentions  # 이 값이 각 층의 attention 가중치\n",
    "\n",
    "# 첫 번째 층의 첫 번째 헤드의 attention 가중치 시각화\n",
    "attention = attentions[0][0][0].detach().numpy()\n",
    "\n",
    "# 토큰 리스트\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "# Attention 가중치 시각화\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens, cmap='viridis')\n",
    "plt.title('Attention Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0c9d8-15fb-4ecb-a077-d8536bf3830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 입력 텍스트\n",
    "input_text = \"오늘 날씨는 \"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# 모델로 다음 단어 예측\n",
    "outputs = model(input_ids)\n",
    "logits = outputs.logits\n",
    "\n",
    "# 가장 높은 확률을 가진 다음 토큰의 ID를 얻습니다.\n",
    "next_token_id = logits[:, -1, :].argmax(dim=-1).item()\n",
    "\n",
    "# 다음 토큰 ID를 텍스트로 변환\n",
    "next_token = tokenizer.decode([next_token_id], skip_special_tokens=True)\n",
    "print(\"다음 단어 예측:\", next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28cbd6d-6fdb-4d4c-822e-d20acc9d7933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
