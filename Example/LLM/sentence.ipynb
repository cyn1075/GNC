{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a9959-795a-470b-b672-9ae7caefd9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 데이터셋 예시\n",
    "data = [\n",
    "    \"Python is a widely used high-level programming language for general-purpose programming.\",\n",
    "    \"Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed.\",\n",
    "    \"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language.\",\n",
    "    \"Deep learning is a class of machine learning algorithms that use multiple layers to progressively extract higher-level features from the raw input.\",\n",
    "    \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals.\",\n",
    "    \"Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data.\",\n",
    "    \"Reinforcement learning is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize some notion of cumulative reward.\",\n",
    "    \"Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos.\",\n",
    "    \"Robotics is an interdisciplinary branch of engineering and science that includes mechanical engineering, electronic engineering, information engineering, computer science, and others.\",\n",
    "    \"Quantum computing is the use of quantum-mechanical phenomena such as superposition and entanglement to perform computation.\"\n",
    "]\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = Dataset.from_dict({\"text\": data})\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "datasets = DatasetDict({\"train\": dataset[\"train\"], \"test\": dataset[\"test\"]})\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 토큰화 함수 정의\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=50)\n",
    "\n",
    "# 데이터셋 토큰화\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# 데이터 로더 설정\n",
    "train_loader = DataLoader(tokenized_datasets[\"train\"], batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(tokenized_datasets[\"test\"], batch_size=2)\n",
    "\n",
    "# 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 장비 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 손실 함수 및 최적화기 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 학습 루프\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = torch.tensor(batch[\"input_ids\"], dtype=torch.long).to(device)  # 예를 들어, dtype을 명시적으로 지정할 수 있습니다.\n",
    "        attention_mask = torch.tensor(batch[\"attention_mask\"], dtype=torch.long).to(device)\n",
    "        # input_ids = torch.tensor(batch[\"input_ids\"]).to(device)\n",
    "        # attention_mask = torch.tensor(batch[\"attention_mask\"]).to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}')\n",
    "\n",
    "# 모델 평가 및 텍스트 생성 함수\n",
    "def generate_text(model, tokenizer, start_text, max_length=50, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(start_text, return_tensors='pt').to(device)\n",
    "    generated_ids = model.generate(input_ids, max_length=max_length, temperature=temperature, pad_token_id=tokenizer.eos_token_id)\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# 문장 생성 테스트\n",
    "start_text = \"Python programming language is\"\n",
    "generated_text = generate_text(model, tokenizer, start_text)\n",
    "print(f'Generated Text: {generated_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5790604-ed14-4d34-a869-cff51076caa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def autocomplete(prompt, max_length=50):\n",
    "    # 입력 텍스트를 토큰화\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # 모델을 사용하여 예측 수행\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # 토큰을 텍스트로 디코딩\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 예제 입력 텍스트\n",
    "prompt = \"The quick brown fox jumps over the\"\n",
    "completed_text = autocomplete(prompt)\n",
    "\n",
    "print(completed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d07267-c119-438f-9162-ff7ca75c1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def autocomplete(prompt, max_length=50):\n",
    "    # 입력 텍스트를 토큰화하고 attention_mask 설정\n",
    "    inputs = tokenizer.encode_plus(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    # 모델을 사용하여 예측 수행\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # 토큰을 텍스트로 디코딩\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 예제 입력 텍스트\n",
    "prompt = \"The quick brown fox jumps over the\"\n",
    "completed_text = autocomplete(prompt)\n",
    "\n",
    "print(completed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f44b5-3c69-42c3-ad91-5cd3f322bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 종료 토큰을 패딩 토큰으로 설정\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def autocomplete(prompt, max_length=50):\n",
    "    # 입력 텍스트를 토큰화하고 attention_mask 설정\n",
    "    inputs = tokenizer.encode_plus(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    # 모델을 사용하여 예측 수행\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # 토큰을 텍스트로 디코딩\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 예제 입력 텍스트\n",
    "prompt = \"?\"\n",
    "completed_text = autocomplete(prompt)\n",
    "\n",
    "print(completed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0d4824-242a-402c-989c-87f6fcea12e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# 샘플 텍스트 데이터\n",
    "text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog\n",
    "The quick brown fox is quick and fast\n",
    "Lazy dogs are often found lying in the sun\n",
    "A brown dog quickly jumps over the lazy fox\n",
    "Foxes are known for their quick movements\n",
    "The sun shines bright and the sky is blue\n",
    "Dogs are loyal and friendly animals\n",
    "Foxes are cunning and intelligent creatures\n",
    "The brown dog and the quick fox are friends\n",
    "In the bright sun, the lazy dog takes a nap\n",
    "\"\"\"\n",
    "\n",
    "# 텍스트 데이터를 토큰화\n",
    "tokenizer = Tokenizer()\n",
    "#fit_on_texts 단어를 인덱스 형태로 변환한다.\n",
    "tokenizer.fit_on_texts([text])\n",
    "\n",
    "# word_index 계산한 인덱스 단어를 dic 형태로 변환한다.\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# 입력 시퀀스 생성\n",
    "input_sequences = []\n",
    "# 반복을 하는데,,, 위에 text형태의 글을 가져와서 \\n으로 짤라서 횟수를 센다.\n",
    "for line in text.split('\\n'):\n",
    "    # texts_to_sequences 텍스트 안의 단어들을 숫자의 시퀀스 형태호 변환\n",
    "    # {'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
    "    # [[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    \n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "        # input_sequence에 어떤 값이 들어갔나 확인중\n",
    "        # print(len(input_sequences))\n",
    "\n",
    "# 패딩을 추가하여 시퀀스의 길이를 맞춤\n",
    "# 최대 길이를 찾아본다. 10 이였음\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "print('max : ', max_sequence_len)\n",
    "\n",
    "# 그리고 빈값에다가 0을 집어넣어서 최대 길이 10을 맞춰준다. \n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "print('input : ', input_sequences)\n",
    "\n",
    "\n",
    "# 입력과 출력 데이터 분리\n",
    "# : 배열의 모든 행을 선택, :-1각 행에서 마지막 요소를 제외한 모든 요소를 선택,\n",
    "# -1 행의 마지막 요소만 선택해서 넣는다.\n",
    "xs, labels = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "# keras 라이브러리 레이블을 원-핫 인코딩 형태로 변환하는데 사용한다.\n",
    "# 정수 형태의 레이블을 이진 벡터로 변환 하는 기법이다.\n",
    "ys = to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "# # 모델 구성\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "# model.add(LSTM(150))\n",
    "# model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# # 모델 컴파일 및 학습\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.fit(xs, ys, epochs=100, verbose=1)\n",
    "\n",
    "# # 단어 자동완성 함수\n",
    "# def autocomplete_text(seed_text, next_words, max_sequence_len):\n",
    "#     for _ in range(next_words):\n",
    "#         token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "#         token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "#         predicted = model.predict(token_list, verbose=0)\n",
    "#         predicted_word_index = np.argmax(predicted, axis=-1)\n",
    "#         output_word = tokenizer.index_word[predicted_word_index[0]]\n",
    "#         seed_text += \" \" + output_word\n",
    "#     return seed_text\n",
    "\n",
    "# # 예제 실행\n",
    "# seed_text = \"The quick brown\"\n",
    "# next_words = 3\n",
    "# completed_text = autocomplete_text(seed_text, next_words, max_sequence_len)\n",
    "# print(completed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66f91c-365b-48b3-94bc-dd92d1e51d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 실행\n",
    "seed_text = \"Dogs\"\n",
    "next_words = 1\n",
    "completed_text = autocomplete_text(seed_text, next_words, max_sequence_len)\n",
    "print(completed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d4e11c-5584-4364-a34c-d65ca412ec69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
